<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->

# Databricks CloudFetch E2E Benchmark

Real end-to-end benchmark for measuring memory usage and performance of the Databricks CloudFetch implementation against an actual Databricks cluster.

## Overview

This benchmark tests the complete CloudFetch flow with real queries against a Databricks warehouse:
- Full end-to-end CloudFetch flow (query execution, downloads, LZ4 decompression, batch consumption)
- Real data from Databricks tables
- Memory usage tracking with actual network I/O
- Power BI consumption simulation with batch-proportional delays (5ms per 10K rows)

## Quick Start - Running Locally

### Prerequisites

Create a JSON config file with your Databricks cluster details:

```json
{
  "uri": "https://your-workspace.cloud.databricks.com/sql/1.0/warehouses/xxx",
  "token": "dapi...",
  "query": "select * from main.tpcds_sf1_delta.catalog_sales",
  "type": "databricks"
}
```

Or use OAuth client credentials:

```json
{
  "uri": "https://your-workspace.cloud.databricks.com/sql/1.0/warehouses/xxx",
  "auth_type": "oauth",
  "grant_type": "client_credentials",
  "client_id": "your-client-id",
  "client_secret": "your-client-secret",
  "query": "select * from main.tpcds_sf1_delta.catalog_sales",
  "type": "databricks"
}
```

### Run the Benchmark

```bash
# Set configuration
export DATABRICKS_TEST_CONFIG_FILE=/path/to/databricks-config.json

# Run via CI script
./ci/scripts/csharp_benchmark.sh $(pwd) net8.0

# Or run directly
cd csharp/Benchmarks
dotnet run -c Release --project DatabricksBenchmarks.csproj --framework net8.0 -- --filter "*CloudFetchRealE2E*"
```

### Understanding Local Results

**Console output during execution:**
```
Loaded config from: /path/to/databricks-config.json
Hostname: adb-xxx.azuredatabricks.net
HTTP Path: /sql/1.0/warehouses/xxx
Query: select * from main.tpcds_sf1_delta.catalog_sales
Benchmark will test CloudFetch with 5ms per 10K rows read delay

// Warmup
CloudFetch E2E [Delay=5ms/10K rows] - Peak memory: 272.97 MB
WorkloadWarmup   1: 1 op, 11566591709.00 ns, 11.5666 s/op

// Actual iterations (3 runs)
CloudFetch E2E [Delay=5ms/10K rows] - Peak memory: 249.11 MB
WorkloadResult   1: 1 op, 8752445353.00 ns, 8.7524 s/op
...
```

**Key Metrics:**
- **Peak Memory (MB)**: Maximum working set memory during execution (lower is better)
- **Allocated**: Total managed memory allocated during the operation
- **Gen0/Gen1/Gen2**: Number of garbage collections (lower is better)
  - Gen0: Frequent, low cost (short-lived objects)
  - Gen1/Gen2: Less frequent, higher cost (longer-lived objects)
- **Mean/Median**: Execution time statistics (includes query execution, CloudFetch downloads, LZ4 decompression, batch consumption)

---

## CI/CD Automation

### Overview

Benchmarks run automatically via GitHub Actions on every commit to the `main` branch:

- **Multi-platform**: .NET 8.0 (Ubuntu) and .NET Framework 4.7.2 (Windows)
- **Performance tracking**: Min Execution Time, Peak Memory, Allocated Memory, and Gen2 Collections
- **Artifact storage**: Detailed results retained for 90 days
- **Trend tracking**: GitHub Pages with interactive performance charts
- **Regression alerts**: Triggers if metrics regress significantly
- **PR benchmarking**: Label-based performance testing with automatic comparison comments

### Viewing CI Results

1. **GitHub Actions**: Go to **Actions â†’ Performance Benchmarks** to see run history
2. **Artifacts**: Download detailed HTML/JSON/CSV reports from each run (90-day retention)
3. **Trend Charts**: View performance trends at https://adbc-drivers.github.io/databricks/bench/

### Manual Trigger

You can manually trigger a benchmark run:

1. Go to **Actions â†’ Performance Benchmarks â†’ Run workflow**
2. Select the `main` branch
3. Optionally specify a custom SQL query
4. Click **Run workflow**

**Note**: Manual triggers require write access to the repository.

### Pull Request Benchmarking (Label-Based)

Test performance impact of your changes **before merging** using label-based PR benchmarks:

#### How to Run Benchmarks on a PR:

1. **Open your PR** (or navigate to an existing PR)
2. **Add the `benchmark` label**:
   - Click "Labels" in the right sidebar
   - Select `benchmark` from the dropdown
   - Save
3. **Wait ~30 minutes** for the workflow to complete
4. **Review the comparison comment** automatically posted on your PR

#### What You Get:

The workflow will automatically post **two separate comments** comparing your PR against the main branch baseline:

**Comment 1: .NET 8.0 Results**
```markdown
## ðŸŽ¯ Benchmark Results (.NET 8.0)

| Metric | Baseline (main) | This PR | Change | Status |
|--------|----------------|---------|--------|--------|
| Min Execution Time (s) | 3.794 | 3.821 | +0.7% | âœ… |
| Peak Memory (MB) | 420.98 | 398.12 | -5.4% | ðŸŸ¢ |
| Allocated Memory (MB) | 286.19 | 275.43 | -3.8% | ðŸŸ¢ |
| Gen2 Collections | 61 | 58 | -4.9% | ðŸŸ¢ |
```

**Comment 2: .NET Framework 4.7.2 Results**
```markdown
## ðŸŽ¯ Benchmark Results (.NET Framework 4.7.2)

| Metric | Baseline (main) | This PR | Change | Status |
|--------|----------------|---------|--------|--------|
| Min Execution Time (s) | 4.123 | 4.201 | +1.9% | âœ… |
| Peak Memory (MB) | 346.19 | 331.03 | -4.4% | ðŸŸ¢ |
| Allocated Memory (MB) | 539.65 | 530.39 | -1.7% | âœ… |
| Gen2 Collections | 7 | 7 | 0.0% | âœ… |
```

**Indicators:**
- ðŸŸ¢ **Improvement** - Metric improved by >30%
- âœ… **No significant change** - Within Â±30%
- âš ï¸ **Regression** - Metric degraded by >30%
- âž– **New metric** - No baseline data available (first run)

#### Key Features:

- âœ… **Opt-in**: Only runs when you add the label (cost-effective)
- âœ… **Automatic comparison**: Shows exact performance impact vs baseline
- âœ… **Non-blocking**: Alerts don't fail the workflow
- âœ… **Accessible**: Any contributor with write access can add labels
- âœ… **No gh-pages pollution**: PR results aren't added to historical tracking

#### Alert Threshold:

The workflow alerts if any metric regresses by **>30%** compared to baseline. This helps identify:
- Performance regressions before merging
- Memory leaks or increased allocations
- Changes that trigger excessive garbage collections

**Note**: Alerts are informational only and won't block your PR. Use them to make informed decisions about performance trade-offs.

---

## CI/CD Setup Guide

### Required GitHub Secrets

Configure these secrets under **Settings â†’ Secrets and variables â†’ Actions â†’ Environment secrets** for the `azure-prod` environment:

- `DATABRICKS_HOST`: Databricks workspace hostname (e.g., `adb-xxx.azuredatabricks.net`)
- `TEST_PECO_WAREHOUSE_HTTP_PATH`: SQL warehouse HTTP path (e.g., `/sql/1.0/warehouses/xxx`)
- `DATABRICKS_TEST_CLIENT_ID`: OAuth client ID for M2M authentication
- `DATABRICKS_TEST_CLIENT_SECRET`: OAuth client secret for M2M authentication

### Enabling Trend Tracking (GitHub Pages)

The workflow uses [`benchmark-action/github-action-benchmark`](https://github.com/benchmark-action/github-action-benchmark) to track performance trends over time.

**Enable GitHub Pages:**

1. Go to **Settings â†’ Pages**
2. Under "Source", select **Deploy from a branch**
3. Select branch: **gh-pages** and folder: **/ (root)**
4. Click **Save**

**Configure GitHub Actions permissions:**

1. Go to **Settings â†’ Actions â†’ General**
2. Under "Workflow permissions", select **Read and write permissions**
3. Check **Allow GitHub Actions to create and approve pull requests**
4. Click **Save**

**First run:**
- The first benchmark run will automatically create the `gh-pages` branch
- Subsequent runs will push benchmark data to this branch
- GitHub Pages will be available at: https://adbc-drivers.github.io/databricks/bench/

---

## Configuration

### Default Benchmark Query

```sql
select * from main.tpcds_sf1_delta.catalog_sales
```

This query processes approximately 1.4M rows and tests CloudFetch with realistic data volumes.

**Using a custom query:**
1. Trigger the workflow manually (Actions â†’ Performance Benchmarks â†’ Run workflow)
2. Enter your custom query in the "Custom SQL query" field
3. The query will be added to the config file automatically

### Alert Thresholds

- **PR benchmarks**: Alert if any metric regresses by >30%
- **Main branch**: Alert if metrics increase by >150% (2.5x baseline)

Alerts are informational only and don't fail the workflow.

### Timeout

Each benchmark job has a 30-minute timeout. Adjust if needed:

```yaml
jobs:
  benchmark-net8:
    timeout-minutes: 30  # Adjust this value
```

---

## Interpreting Results

### Artifacts

After each CI run, detailed results are uploaded as artifacts (90-day retention):

- `benchmark-results-net8`: Results from .NET 8.0 benchmark
- `benchmark-results-net472`: Results from .NET Framework 4.7.2 benchmark

Each artifact contains:
- **JSON report** (`*-report-full-compressed.json`): Complete benchmark results with detailed metrics used for trend tracking

### Key Metrics

**Tracked in GitHub Pages (trend analysis):**

1. **Min Execution Time (seconds)**: Best (fastest) execution time across benchmark iterations
   - Primary performance metric tracked on GitHub Pages
   - End-to-end time including query execution, CloudFetch downloads, LZ4 decompression, and batch consumption
   - Lower is better (tracks performance improvements)
   - Alert threshold: 130% on PRs (30% regression warning)
   - Source: BenchmarkDotNet's `Statistics.Min`

2. **Peak Memory (MB)**: Maximum working set memory (private bytes) during execution
   - Lower is better
   - Alert threshold: 150% on main, 130% on PRs
   - Source: Custom metrics from `Process.PrivateMemorySize64`

3. **Allocated Memory (MB)**: Total managed memory allocated during execution
   - Lower is better
   - Alert threshold: 150% on main, 130% on PRs
   - Source: BenchmarkDotNet's `MemoryDiagnoser`

4. **Gen2 Collections**: Number of full garbage collections
   - Lower is better (indicates less memory pressure)
   - Alert threshold: 150%
   - Source: BenchmarkDotNet's `MemoryDiagnoser`

**Additional metrics in BenchmarkDotNet reports:**

5. **Total Rows/Batches**: Data volume processed

6. **GC Time %**: Percentage of time spent in garbage collection

7. **Gen0/Gen1 Collections**: Minor and partial garbage collections

### Trend Analysis

The GitHub Pages dashboard shows:
- **Execution time trends**: Line chart of Mean/Min/Max execution time across commits
- **Memory usage trends**: Line chart of Peak Memory, Allocated Memory, and Gen2 Collections across commits
- **Regression detection**: Automatic alerts when performance degrades (memory increases by 150%+)
- **Comparison view**: Compare performance between different commits

---

## Troubleshooting

### Workflow Issues

**Issue**: Workflow fails with "Config file not found"
- **Solution**: Verify GitHub secrets are properly configured in the `azure-prod` environment

**Issue**: Benchmark times out (>30 minutes)
- **Solution**: Increase `timeout-minutes` value in the workflow or use a smaller test query

**Issue**: Trend tracking not working
- **Solution**: Ensure GitHub Pages is enabled and workflow has write permissions

**Issue**: Results not showing in GitHub Pages
- **Solution**: Wait a few minutes after the first run for Pages to deploy, then check the `gh-pages` branch

### Local Execution Issues

**Issue**: "DATABRICKS_TEST_CONFIG_FILE environment variable must be set"
- **Solution**: Set the environment variable: `export DATABRICKS_TEST_CONFIG_FILE=/path/to/config.json`

**Issue**: Authentication fails
- **Solution**: Verify your token or OAuth credentials are correct in the config file

---

## Frequently Asked Questions

### Does the benchmark run on every commit to main?

**No** - Benchmarks only run when relevant files change:

```yaml
paths:
  - '.github/workflows/benchmarks.yml'  # Workflow changes
  - 'csharp/src/**'                     # Driver source code
  - 'csharp/Benchmarks/**'              # Benchmark code
```

**Runs when:**
- âœ… You change driver source code
- âœ… You change benchmark code
- âœ… You modify the workflow itself

**Doesn't run when:**
- âŒ Documentation-only changes
- âŒ Test-only changes (unless they affect src/)
- âŒ Changes to unrelated parts of the repository

This selective triggering saves CI resources and provides faster feedback for non-performance changes.

### Do performance alerts block check-ins?

**No** - Alerts are informational only and do **not** block merges or fail the workflow.

**For PRs:** If metrics regress by >30%, a warning (âš ï¸) appears in the comparison comment but doesn't fail the workflow.

**For main branch:** If metrics regress by >150%, an alert appears in GitHub Pages dashboard but the workflow still passes.

**Why non-blocking?**
- Critical bug fixes shouldn't be blocked by performance alerts
- Performance trade-offs are sometimes intentional
- Alerts provide visibility while allowing informed decisions

### How do I investigate a performance regression?

When an alert triggers:

1. **Check GitHub Pages dashboard** - View the trend chart to see the increase
2. **Download artifacts** - Get the detailed benchmark reports from the workflow run
3. **Compare commits** - Use the comparison view to see what changed
4. **Review the code** - Look at the changes that caused the regression
5. **Assess trade-off** - Determine if the regression is acceptable
6. **Create follow-up** - If needed, create a PR to optimize performance

---

## Best Practices

1. **Run benchmarks on stable infrastructure**: CI uses GitHub-hosted runners which may have variable performance
2. **Use consistent test data**: The default TPC-DS query provides consistent results across runs
3. **Monitor trends, not absolute values**: Focus on relative changes over time
4. **Review alerts promptly**: Performance regressions should be investigated quickly
5. **Keep benchmarks fast**: Long-running benchmarks slow down the CI pipeline

---

## References

- [BenchmarkDotNet Documentation](https://benchmarkdotnet.org/)
- [github-action-benchmark](https://github.com/benchmark-action/github-action-benchmark)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)
